""" 
What Does the Script Do?
  Crawls Tor Project pages to find IP address lists generated today.
  Cleans and deduplicates the data.
  Saves the unique IPs into a CSV file with relevant metadata.
""" 

"""
Use Cases
  Cybersecurity: Identifying Tor exit nodes or relays.
  Threat Intelligence: Monitoring IPs for suspicious activities.
  Automation: Daily collection and analysis of Tor network data.
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from urllib.parse import urljoin
import re
from datetime import datetime, date


def extract_date_from_url(url):
    # Check for the format 'YYYY-MM-DD-HH-MM-SS'
    date_match = re.search(r'(\d{4}-\d{2}-\d{2})-\d{2}-\d{2}-\d{2}', url)
    if date_match:
        return datetime.strptime(date_match.group(1), '%Y-%m-%d').date()

    # Check for the format 'YYYY-MM-DD'
    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', url)
    if date_match:
        return datetime.strptime(date_match.group(1), '%Y-%m-%d').date()

    return None


def download_ip_list(url):
    response = requests.get(url)
    if response.status_code == 200:
        return response.text.splitlines()
    else:
        print(f"Failed to download from {url}")
        return []


def process_page(url, target_date):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    download_links = []
    for a in soup.find_all('a', href=True):
        full_url = urljoin(url, a['href'])
        link_date = extract_date_from_url(full_url)
        if link_date and link_date == target_date:
            download_links.append(full_url)

    return download_links


def clean_ip_data(ip_list, is_consensus_microdesc=False):
    cleaned_ips = set()  # Using a set to automatically remove duplicates
    is_exit = False
    current_ip = None

    for line in ip_list:
        if is_consensus_microdesc:
            if line.startswith('r '):
                # Extract IP from the 'r' line
                parts = line.split()
                if len(parts) >= 7:
                    current_ip = parts[6]
            elif line.startswith('a '):
                # If there's an 'a' line, it contains the IPv6 address
                parts = line.split()
                if len(parts) >= 2:
                    current_ip = parts[1].strip('[]')
            elif line.startswith('s '):
                # Check if this is an exit node
                is_exit = 'Exit' in line
                if is_exit and current_ip:
                    cleaned_ips.add(current_ip)
                current_ip = None
                is_exit = False
        else:
            # For exit-lists file
            if line.startswith('ExitAddress'):
                parts = line.split()
                if len(parts) >= 2:
                    cleaned_ips.add(parts[1])

    # Clean the IP addresses
    final_ips = set()
    for ip in cleaned_ips:
        # Remove port numbers and brackets
        ip = re.sub(r':\d+$', '', ip)
        ip = ip.strip('[]')

        # Remove any remaining non-IP characters
        ip = re.sub(r'[^0-9a-fA-F:.]', '', ip)

        # Validate IPv4 and IPv6 addresses
        if re.match(r'^(\d{1,3}\.){3}\d{1,3}$', ip) or ':' in ip:
            final_ips.add(ip)

    return list(final_ips)


# URLs of the two web pages
urls = [
    "https://collector.torproject.org/recent/exit-lists/",
    "https://metrics.torproject.org/collector/recent/relay-descriptors/microdescs/consensus-microdesc/"
]

# Get the current date
target_date = date.today()

all_ip_data = []

for url in urls:
    download_links = process_page(url, target_date)

    for link in download_links:
        ip_list = download_ip_list(link)
        is_consensus_microdesc = 'consensus-microdesc' in link
        cleaned_ips = clean_ip_data(ip_list, is_consensus_microdesc)
        for ip in cleaned_ips:
            all_ip_data.append({
                'IP': ip,
                'Date': target_date,
                'Source': url
            })

# Create a pandas DataFrame
df = pd.DataFrame(all_ip_data)

# Remove any remaining duplicates (if any)
df.drop_duplicates(subset=['IP'], keep='first', inplace=True)

# Save to CSV
csv_filename = f'ip_database_{target_date}.csv'
df.to_csv(csv_filename, index=False)

print(f"IP database for {target_date} has been created and saved to '{csv_filename}'")
if df.empty:
    print("No data was found for today's date.")
else:
    print(f"Number of unique IP addresses collected: {len(df)}")
